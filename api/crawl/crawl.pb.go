// Code generated by protoc-gen-go. DO NOT EDIT.
// source: crawl.proto

package crawl

import proto "github.com/golang/protobuf/proto"
import fmt "fmt"
import math "math"

import (
	context "golang.org/x/net/context"
	grpc "google.golang.org/grpc"
)

// Reference imports to suppress errors if they are not otherwise used.
var _ = proto.Marshal
var _ = fmt.Errorf
var _ = math.Inf

// This is a compile-time assertion to ensure that this generated file
// is compatible with the proto package it is being compiled against.
// A compilation error at this line likely means your copy of the
// proto package needs to be updated.
const _ = proto.ProtoPackageIsVersion2 // please upgrade the proto package

type URLRequestCommand int32

const (
	// URLs in STOPPED, NONE, or DONE may be started.
	URLRequest_START URLRequestCommand = 0
	// URLs in CRAWLING, STOPPED, or DONE may be stopped.
	URLRequest_STOP URLRequestCommand = 1
	// URLs in any state may be checked.
	URLRequest_CHECK URLRequestCommand = 2
)

var URLRequestCommand_name = map[int32]string{
	0: "START",
	1: "STOP",
	2: "CHECK",
}
var URLRequestCommand_value = map[string]int32{
	"START": 0,
	"STOP":  1,
	"CHECK": 2,
}

func (x URLRequestCommand) String() string {
	return proto.EnumName(URLRequestCommand_name, int32(x))
}
func (URLRequestCommand) EnumDescriptor() ([]byte, []int) {
	return fileDescriptor_crawl_d76c9fbb24143501, []int{0, 0}
}

type URLStateUrlStatus int32

const (
	URLState_STOPPED URLStateUrlStatus = 0
	// START for a STOPPED URL resumes the crawl.
	// STOP for a STOPPED URL does nothing.
	URLState_RUNNING URLStateUrlStatus = 1
	// Once it completes the crawl, it switches
	// the URL's state to DONE. START for a
	// CRAWLING URL is a no-op. STOP for a CRAWLING
	// URL saves the URL's state and sets it to STOPPED.
	URLState_DONE URLStateUrlStatus = 2
	// If the crawler receives a START for a DONE
	// URL, it discards the crawl history and
	// crawls it again. If it receives a STOP, it
	// does nothing.
	URLState_UNKNOWN URLStateUrlStatus = 3
	// This is a meta-state; URLs never crawled
	// are not recorded in the client to avoid a
	// possible DoS from a clog of never-crawled URLs.
	// Only returned for a STOP.
	URLState_FAILED URLStateUrlStatus = 4
)

var URLStateUrlStatus_name = map[int32]string{
	0: "STOPPED",
	1: "RUNNING",
	2: "DONE",
	3: "UNKNOWN",
	4: "FAILED",
}
var URLStateUrlStatus_value = map[string]int32{
	"STOPPED": 0,
	"RUNNING": 1,
	"DONE":    2,
	"UNKNOWN": 3,
	"FAILED":  4,
}

func (x URLStateUrlStatus) String() string {
	return proto.EnumName(URLStateUrlStatus_name, int32(x))
}
func (URLStateUrlStatus) EnumDescriptor() ([]byte, []int) {
	return fileDescriptor_crawl_d76c9fbb24143501, []int{1, 0}
}

// URLRequest defines the outgoing request.
// We can provide a URL and the state we want the client
// to put it in.
type URLRequest struct {
	URL                  string            `protobuf:"bytes,1,opt,name=URL,proto3" json:"URL,omitempty"`
	State                URLRequestCommand `protobuf:"varint,2,opt,name=state,proto3,enum=crawl.URLRequestCommand" json:"state,omitempty"`
	XXX_NoUnkeyedLiteral struct{}          `json:"-"`
	XXX_unrecognized     []byte            `json:"-"`
	XXX_sizecache        int32             `json:"-"`
}

func (m *URLRequest) Reset()         { *m = URLRequest{} }
func (m *URLRequest) String() string { return proto.CompactTextString(m) }
func (*URLRequest) ProtoMessage()    {}
func (*URLRequest) Descriptor() ([]byte, []int) {
	return fileDescriptor_crawl_d76c9fbb24143501, []int{0}
}
func (m *URLRequest) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_URLRequest.Unmarshal(m, b)
}
func (m *URLRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_URLRequest.Marshal(b, m, deterministic)
}
func (dst *URLRequest) XXX_Merge(src proto.Message) {
	xxx_messageInfo_URLRequest.Merge(dst, src)
}
func (m *URLRequest) XXX_Size() int {
	return xxx_messageInfo_URLRequest.Size(m)
}
func (m *URLRequest) XXX_DiscardUnknown() {
	xxx_messageInfo_URLRequest.DiscardUnknown(m)
}

var xxx_messageInfo_URLRequest proto.InternalMessageInfo

func (m *URLRequest) GetURL() string {
	if m != nil {
		return m.URL
	}
	return ""
}

func (m *URLRequest) GetState() URLRequestCommand {
	if m != nil {
		return m.State
	}
	return URLRequest_START
}

// URLState reports the crawl status ONLY of a URL.
type URLState struct {
	Status               URLStateUrlStatus `protobuf:"varint,1,opt,name=status,proto3,enum=crawl.URLStateUrlStatus" json:"status,omitempty"`
	XXX_NoUnkeyedLiteral struct{}          `json:"-"`
	XXX_unrecognized     []byte            `json:"-"`
	XXX_sizecache        int32             `json:"-"`
}

func (m *URLState) Reset()         { *m = URLState{} }
func (m *URLState) String() string { return proto.CompactTextString(m) }
func (*URLState) ProtoMessage()    {}
func (*URLState) Descriptor() ([]byte, []int) {
	return fileDescriptor_crawl_d76c9fbb24143501, []int{1}
}
func (m *URLState) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_URLState.Unmarshal(m, b)
}
func (m *URLState) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_URLState.Marshal(b, m, deterministic)
}
func (dst *URLState) XXX_Merge(src proto.Message) {
	xxx_messageInfo_URLState.Merge(dst, src)
}
func (m *URLState) XXX_Size() int {
	return xxx_messageInfo_URLState.Size(m)
}
func (m *URLState) XXX_DiscardUnknown() {
	xxx_messageInfo_URLState.DiscardUnknown(m)
}

var xxx_messageInfo_URLState proto.InternalMessageInfo

func (m *URLState) GetStatus() URLStateUrlStatus {
	if m != nil {
		return m.Status
	}
	return URLState_STOPPED
}

// SiteNode is returned in response to a STATUS request.
// It returns a tree of sitenodes found under the current
// URL (which may recursively contain more SiteNodes).
// If no URL is supplied, all the SiteNodes the crawler
// knows about are returned as the children of a SiteNode
// with the siteURL "all://".
type SiteNode struct {
	SiteURL              string   `protobuf:"bytes,1,opt,name=siteURL,proto3" json:"siteURL,omitempty"`
	TreeString           string   `protobuf:"bytes,2,opt,name=treeString,proto3" json:"treeString,omitempty"`
	Status               string   `protobuf:"bytes,3,opt,name=status,proto3" json:"status,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *SiteNode) Reset()         { *m = SiteNode{} }
func (m *SiteNode) String() string { return proto.CompactTextString(m) }
func (*SiteNode) ProtoMessage()    {}
func (*SiteNode) Descriptor() ([]byte, []int) {
	return fileDescriptor_crawl_d76c9fbb24143501, []int{2}
}
func (m *SiteNode) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_SiteNode.Unmarshal(m, b)
}
func (m *SiteNode) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_SiteNode.Marshal(b, m, deterministic)
}
func (dst *SiteNode) XXX_Merge(src proto.Message) {
	xxx_messageInfo_SiteNode.Merge(dst, src)
}
func (m *SiteNode) XXX_Size() int {
	return xxx_messageInfo_SiteNode.Size(m)
}
func (m *SiteNode) XXX_DiscardUnknown() {
	xxx_messageInfo_SiteNode.DiscardUnknown(m)
}

var xxx_messageInfo_SiteNode proto.InternalMessageInfo

func (m *SiteNode) GetSiteURL() string {
	if m != nil {
		return m.SiteURL
	}
	return ""
}

func (m *SiteNode) GetTreeString() string {
	if m != nil {
		return m.TreeString
	}
	return ""
}

func (m *SiteNode) GetStatus() string {
	if m != nil {
		return m.Status
	}
	return ""
}

func init() {
	proto.RegisterType((*URLRequest)(nil), "crawl.URLRequest")
	proto.RegisterType((*URLState)(nil), "crawl.URLState")
	proto.RegisterType((*SiteNode)(nil), "crawl.SiteNode")
	proto.RegisterEnum("crawl.URLRequestCommand", URLRequestCommand_name, URLRequestCommand_value)
	proto.RegisterEnum("crawl.URLStateUrlStatus", URLStateUrlStatus_name, URLStateUrlStatus_value)
}

// Reference imports to suppress errors if they are not otherwise used.
var _ context.Context
var _ grpc.ClientConn

// This is a compile-time assertion to ensure that this generated file
// is compatible with the grpc package it is being compiled against.
const _ = grpc.SupportPackageIsVersion4

// CrawlClient is the client API for Crawl service.
//
// For semantics around ctx use and closing/ending streaming RPCs, please refer to https://godoc.org/google.golang.org/grpc#ClientConn.NewStream.
type CrawlClient interface {
	// Because we're calling the client from our CLI, we
	// want the CrawlSite API to make a single request
	// and wait for the response. This API lets us start,
	// stop, or check the status of a URL
	CrawlSite(ctx context.Context, in *URLRequest, opts ...grpc.CallOption) (*URLState, error)
	// Checks the current status of a crawl and returns
	// the tree as it stands.
	CrawlResult(ctx context.Context, in *URLRequest, opts ...grpc.CallOption) (*SiteNode, error)
}

type crawlClient struct {
	cc *grpc.ClientConn
}

func NewCrawlClient(cc *grpc.ClientConn) CrawlClient {
	return &crawlClient{cc}
}

func (c *crawlClient) CrawlSite(ctx context.Context, in *URLRequest, opts ...grpc.CallOption) (*URLState, error) {
	out := new(URLState)
	err := c.cc.Invoke(ctx, "/crawl.Crawl/CrawlSite", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *crawlClient) CrawlResult(ctx context.Context, in *URLRequest, opts ...grpc.CallOption) (*SiteNode, error) {
	out := new(SiteNode)
	err := c.cc.Invoke(ctx, "/crawl.Crawl/CrawlResult", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

// CrawlServer is the server API for Crawl service.
type CrawlServer interface {
	// Because we're calling the client from our CLI, we
	// want the CrawlSite API to make a single request
	// and wait for the response. This API lets us start,
	// stop, or check the status of a URL
	CrawlSite(context.Context, *URLRequest) (*URLState, error)
	// Checks the current status of a crawl and returns
	// the tree as it stands.
	CrawlResult(context.Context, *URLRequest) (*SiteNode, error)
}

func RegisterCrawlServer(s *grpc.Server, srv CrawlServer) {
	s.RegisterService(&_Crawl_serviceDesc, srv)
}

func _Crawl_CrawlSite_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(URLRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(CrawlServer).CrawlSite(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/crawl.Crawl/CrawlSite",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(CrawlServer).CrawlSite(ctx, req.(*URLRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _Crawl_CrawlResult_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(URLRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(CrawlServer).CrawlResult(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/crawl.Crawl/CrawlResult",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(CrawlServer).CrawlResult(ctx, req.(*URLRequest))
	}
	return interceptor(ctx, in, info, handler)
}

var _Crawl_serviceDesc = grpc.ServiceDesc{
	ServiceName: "crawl.Crawl",
	HandlerType: (*CrawlServer)(nil),
	Methods: []grpc.MethodDesc{
		{
			MethodName: "CrawlSite",
			Handler:    _Crawl_CrawlSite_Handler,
		},
		{
			MethodName: "CrawlResult",
			Handler:    _Crawl_CrawlResult_Handler,
		},
	},
	Streams:  []grpc.StreamDesc{},
	Metadata: "crawl.proto",
}

func init() { proto.RegisterFile("crawl.proto", fileDescriptor_crawl_d76c9fbb24143501) }

var fileDescriptor_crawl_d76c9fbb24143501 = []byte{
	// 319 bytes of a gzipped FileDescriptorProto
	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0x7c, 0x51, 0x4d, 0x4f, 0xc2, 0x40,
	0x14, 0xec, 0x02, 0x05, 0xfa, 0x48, 0x74, 0x7d, 0x07, 0x83, 0x1e, 0x0c, 0xd9, 0x13, 0x5e, 0x6a,
	0x80, 0x5f, 0x40, 0xa0, 0x0a, 0xa1, 0x59, 0xc8, 0x2e, 0x8d, 0x17, 0x2f, 0x08, 0x1b, 0xd3, 0xa4,
	0x50, 0x6d, 0xb7, 0x31, 0xf1, 0x0f, 0xf8, 0xb7, 0xcd, 0x2e, 0x05, 0xfc, 0x48, 0xbc, 0xbd, 0x99,
	0x9d, 0x79, 0x93, 0x79, 0x0b, 0xad, 0x75, 0xb6, 0x7a, 0x4f, 0xfc, 0xd7, 0x2c, 0xd5, 0x29, 0xba,
	0x16, 0xb0, 0x0f, 0x80, 0x48, 0x84, 0x42, 0xbd, 0x15, 0x2a, 0xd7, 0x48, 0xa1, 0x1a, 0x89, 0xb0,
	0x4d, 0x3a, 0xa4, 0xeb, 0x09, 0x33, 0xe2, 0x1d, 0xb8, 0xb9, 0x5e, 0x69, 0xd5, 0xae, 0x74, 0x48,
	0xf7, 0xac, 0x7f, 0xe5, 0xef, 0x77, 0x9c, 0x3c, 0xfe, 0x3a, 0xdd, 0x6e, 0x57, 0xbb, 0x8d, 0xd8,
	0xeb, 0xd8, 0x2d, 0x34, 0x4a, 0x06, 0x3d, 0x70, 0xe5, 0x72, 0x28, 0x96, 0xd4, 0xc1, 0x26, 0xd4,
	0xe4, 0x72, 0xbe, 0xa0, 0xc4, 0x90, 0xa3, 0x49, 0x30, 0x9a, 0xd1, 0x0a, 0xfb, 0x24, 0xd0, 0x8c,
	0x44, 0x28, 0x8d, 0x0f, 0x7b, 0x50, 0x37, 0x0b, 0x8a, 0xdc, 0xa6, 0xff, 0x48, 0xb2, 0x02, 0xbf,
	0xc8, 0x12, 0x69, 0x05, 0xa2, 0x14, 0xb2, 0x09, 0x78, 0x47, 0x12, 0x5b, 0xd0, 0x30, 0x09, 0x8b,
	0x60, 0x4c, 0x1d, 0x03, 0x44, 0xc4, 0xf9, 0x94, 0x3f, 0x50, 0x62, 0xb2, 0xc7, 0x73, 0x1e, 0xd0,
	0x8a, 0xa1, 0x23, 0x3e, 0xe3, 0xf3, 0x47, 0x4e, 0xab, 0x08, 0x50, 0xbf, 0x1f, 0x4e, 0xc3, 0x60,
	0x4c, 0x6b, 0xec, 0x09, 0x9a, 0x32, 0xd6, 0x8a, 0xa7, 0x1b, 0x85, 0x6d, 0x68, 0xe4, 0xb1, 0x56,
	0xa7, 0x3b, 0x1c, 0x20, 0xde, 0x00, 0xe8, 0x4c, 0x29, 0xa9, 0xb3, 0x78, 0xf7, 0x62, 0x0f, 0xe2,
	0x89, 0x6f, 0x0c, 0x5e, 0x1e, 0x2b, 0x54, 0xed, 0x5b, 0x89, 0xfa, 0x29, 0xb8, 0x23, 0xd3, 0x05,
	0x7b, 0xe0, 0xd9, 0xc1, 0x64, 0xe1, 0xc5, 0x9f, 0x53, 0x5e, 0x9f, 0xff, 0xea, 0xcc, 0x1c, 0x1c,
	0x40, 0xcb, 0x5a, 0x84, 0xca, 0x8b, 0x44, 0xff, 0x67, 0x3a, 0x14, 0x60, 0xce, 0x73, 0xdd, 0x7e,
	0xf1, 0xe0, 0x2b, 0x00, 0x00, 0xff, 0xff, 0xc8, 0xf2, 0x08, 0x78, 0xf1, 0x01, 0x00, 0x00,
}
